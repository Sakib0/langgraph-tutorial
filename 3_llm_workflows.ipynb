{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f095536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a95ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Load HUGGINGFACE_API_KEY from environment \n",
    "api_key = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not api_key:\n",
    "    raise RuntimeError('HUGGINGFACEHUB_API_TOKEN not set. Add it to .env or set the environment variable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbb5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize Endpoint with the correct task\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "    task=\"text-generation\",  \n",
    "    \n",
    ")\n",
    "\n",
    "# 3. Wrap in Chat Interface\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a73c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the state\n",
    "class LLMWorkflow(TypedDict):\n",
    "    qus: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b01542ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_qa(state: LLMWorkflow) -> LLMWorkflow:\n",
    "    # Extract the question from the state\n",
    "    question=state['qus']\n",
    "    # Define the prompt template\n",
    "    prompt = (f\"Answer the question: {question}\")\n",
    "    # Get the answer from the model\n",
    "    answer=model.invoke(prompt).content\n",
    "\n",
    "    # Update the state with the answer\n",
    "    state['answer']=answer\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bfca051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the graph\n",
    "graph=StateGraph(LLMWorkflow)\n",
    "\n",
    "# add nodes\n",
    "graph.add_node('llm_qa',llm_qa)\n",
    "\n",
    "# add edges\n",
    "graph.add_edge(START,'llm_qa')\n",
    "graph.add_edge('llm_qa',END)\n",
    "# compile the graph\n",
    "workflow=graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac477168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the capital of France. This seems like a straightforward geography question. \n",
      "\n",
      "Hmm, the answer is definitely Paris - that's basic world knowledge. But I wonder if the user is testing me with such a simple question? Maybe they're a student doing homework or someone verifying facts. \n",
      "\n",
      "I recall that France has other important cities like Lyon or Marseille, but Paris remains both the political and cultural capital. The Eiffel Tower imagery would help reinforce the answer visually. \n",
      "\n",
      "Should I add more context? No, keeping it concise seems best unless they follow up. The flag emoji gives it a nice touch though - shows I'm not just copying from a textbook. \n",
      "\n",
      "The response feels complete: clear answer, visual confirmation, and openness to further questions. Simple but effective.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**.\n",
      "\n",
      "It is the country's political, cultural, and economic center, home to iconic landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. ðŸ‡«ðŸ‡·\n",
      "\n",
      "Let me know if you'd like more information about Paris or France!\n"
     ]
    }
   ],
   "source": [
    "initial_state={'qus':'What is the capital of France?'}\n",
    "final_state=workflow.invoke(initial_state)\n",
    "print(final_state['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64bf61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
